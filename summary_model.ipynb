{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. load model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## reference\n",
    "\n",
    "`load pretrained TransformerSum models` [github](https://github.com/HHousen/TransformerSum) | [tutorial](https://transformersum.readthedocs.io/en/latest/general/getting-started.html#install)\n",
    "\n",
    "\n",
    "1. `distilbert-base-uncased-ext-sum(CNN/DM): 6h 22m 32s` [model download](https://drive.google.com/uc?id=1VNoFhqfwlvgwKuJwjlHnlGcGg38cGM--)\n",
    "    \n",
    "    > Currently, distilbert beats bert-base-uncased by 1.0014% Since bert-base-uncased has more parameters than distilbert, this is unusual and is likely a tuning issue. This suggests that tuning the hyperparameters of bert-base-uncased can improve its performance. distilroberta matches 92.7% of the performance of roberta-base\n",
    "\n",
    "2. `bert-base-uncased-ext-sum(CNN/DM): 12h 51m 17s` [model download](https://drive.google.com/uc?id=1yGvarxhq78Vl6m8IZgG9HFQC2qXDB-KU)\n",
    "\n",
    "3. `mobilebert-uncased-ext-sum(CNN/DM): 8h 26m 32s` [model download](https://drive.google.com/uc?id=1R3tRH07z_9nYW8sC8eFceBmxC7u0kP_W)\n",
    "\n",
    "    > mobilebert-uncased-ext-sum achieves 96.59%  of the performance of BertSum while containing 4.45 times fewer parameters. It achieves 94.06%  of the performance of MatchSum the current extractive state-of-the-art.\n",
    "\n",
    "4. `distilroberta-base-ext-sum(WikiHow): 4h 27m 23s` [model download](https://drive.google.com/uc?id=1RdFcoeuHd_JCj5gBQRFXFpieb-3EXkiN)\n",
    "    \n",
    "    > These are the results of an extractive model, which means they are fairly good because they come close to abstractive models. The R1/R2/RL-Sum results of a base transformer model from the PEGASUS paper are 32.48/10.53/23.86. The net difference from distilroberta-base-ext-sum is +1.41/+1.57/-5.09. Compared to the abstractive SOTA prior to PEGASUS, which was 28.53/9.23/26.54, distilroberta-base-ext-sum performs +2.54/-0.27/+2.41. \n",
    "    \n",
    "    However, the base PEGASUS model obtains scores of 36.58/15.64/30.01, which are much better than distilroberta-base-ext-sum, as one would expect.\n",
    "\n",
    "5. `bert-base-uncased-ext-sum(WikiHow): 7h 29m 06s` [model download](https://drive.google.com/uc?id=1EPCaQySWJgm368XypDeCwEMdRCxB5w7Z)\n",
    "\n",
    "`check architecture, parameters` [blog](https://rabo0313.tistory.com/entry/Pytorch-%EB%AA%A8%EB%8D%B8-%EA%B5%AC%EC%A1%B0-%ED%99%95%EC%9D%B8-parameter%ED%99%95%EC%9D%B8)\n",
    "\n",
    "`[error] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRETRAINED_MODEL_PATH=\"./models/cnn dm [bert-base-uncased-ext-sum] epoch=3.ckpt\"\n",
    "# PRETRAINED_MODEL_PATH=\"./models/cnn dm [distilroberta-base-ext-sum] epoch=3.ckpt\"\n",
    "PRETRAINED_MODEL_PATH=\"./models/cnn dm [mobilebert-uncased-ext-sum] epoch=3.ckpt\"\n",
    "# PRETRAINED_MODEL_PATH=\"./models/wikihow [bert-base-uncased-ext-sum] epoch=2.ckpt\"\n",
    "# PRETRAINED_MODEL_PATH=\"./models/wikihow [distilbert-base-uncased-ext-sum] epoch=3.ckpt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.extractive import ExtractiveSummarizer\n",
    "import torch\n",
    "from torch import nn\n",
    "# !pip install torchsummary\n",
    "from torchsummary import summary as summary_\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ExtractiveSummarizer.load_from_checkpoint(PRETRAINED_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExtractiveSummarizer(\n",
      "  (word_embedding_model): MobileBertModel(\n",
      "    (embeddings): MobileBertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 512)\n",
      "      (token_type_embeddings): Embedding(2, 512)\n",
      "      (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
      "      (LayerNorm): NoNorm()\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (encoder): MobileBertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (6): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (7): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (8): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (9): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (10): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (11): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (12): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (13): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (14): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (15): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (16): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (17): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (18): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (19): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (20): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (21): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (22): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (23): MobileBertLayer(\n",
      "          (attention): MobileBertAttention(\n",
      "            (self): MobileBertSelfAttention(\n",
      "              (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): MobileBertSelfOutput(\n",
      "              (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (intermediate): MobileBertIntermediate(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (intermediate_act_fn): ReLU()\n",
      "          )\n",
      "          (output): MobileBertOutput(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (bottleneck): OutputBottleneck(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "              (dropout): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (bottleneck): Bottleneck(\n",
      "            (input): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "            (attention): BottleneckLayer(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (ffn): ModuleList(\n",
      "            (0): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (1): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "            (2): FFNLayer(\n",
      "              (intermediate): MobileBertIntermediate(\n",
      "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "                (intermediate_act_fn): ReLU()\n",
      "              )\n",
      "              (output): FFNOutput(\n",
      "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "                (LayerNorm): NoNorm()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): MobileBertPooler()\n",
      "  )\n",
      "  (pooling_model): Pooling()\n",
      "  (encoder): SimpleLinearClassifier(\n",
      "    (linear): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (loss_func): BCEWithLogitsLoss()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check model parameters\n",
    "- parameters(): size, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30522, 128]): word_embedding_model.embeddings.word_embeddings.weight\n",
      "torch.Size([512, 512]): word_embedding_model.embeddings.position_embeddings.weight\n",
      "torch.Size([2, 512]): word_embedding_model.embeddings.token_type_embeddings.weight\n",
      "torch.Size([512, 384]): word_embedding_model.embeddings.embedding_transformation.weight\n",
      "torch.Size([512]): word_embedding_model.embeddings.embedding_transformation.bias\n",
      "torch.Size([512]): word_embedding_model.embeddings.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.embeddings.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.0.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.0.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.0.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.0.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.0.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.0.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.0.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.0.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.0.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.0.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.1.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.1.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.1.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.1.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.1.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.1.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.1.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.1.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.1.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.1.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.2.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.2.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.2.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.2.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.2.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.2.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.2.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.2.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.2.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.2.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.3.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.3.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.3.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.3.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.3.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.3.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.3.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.3.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.3.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.3.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.3.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.4.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.4.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.4.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.4.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.4.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.4.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.4.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.4.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.4.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.4.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.4.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.5.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.5.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.5.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.5.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.5.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.5.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.5.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.5.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.5.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.5.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.5.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.6.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.6.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.6.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.6.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.6.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.6.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.6.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.6.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.6.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.6.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.6.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.7.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.7.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.7.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.7.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.7.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.7.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.7.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.7.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.7.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.7.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.7.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.8.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.8.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.8.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.8.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.8.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.8.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.8.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.8.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.8.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.8.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.8.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.9.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.9.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.9.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.9.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.9.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.9.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.9.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.9.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.9.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.9.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.9.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.10.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.10.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.10.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.10.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.10.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.10.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.10.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.10.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.10.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.10.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.10.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.11.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.11.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.11.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.11.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.11.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.11.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.11.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.11.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.11.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.11.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.11.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.12.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.12.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.12.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.12.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.12.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.12.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.12.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.12.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.12.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.12.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.12.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.13.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.13.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.13.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.13.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.13.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.13.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.13.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.13.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.13.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.13.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.13.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.14.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.14.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.14.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.14.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.14.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.14.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.14.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.14.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.14.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.14.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.14.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.15.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.15.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.15.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.15.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.15.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.15.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.15.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.15.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.15.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.15.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.15.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.16.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.16.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.16.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.16.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.16.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.16.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.16.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.16.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.16.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.16.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.16.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.17.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.17.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.17.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.17.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.17.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.17.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.17.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.17.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.17.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.17.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.17.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.18.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.18.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.18.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.18.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.18.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.18.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.18.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.18.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.18.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.18.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.18.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.19.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.19.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.19.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.19.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.19.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.19.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.19.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.19.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.19.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.19.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.19.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.20.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.20.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.20.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.20.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.20.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.20.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.20.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.20.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.20.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.20.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.20.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.21.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.21.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.21.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.21.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.21.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.21.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.21.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.21.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.21.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.21.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.21.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.22.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.22.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.22.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.22.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.22.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.22.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.22.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.22.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.22.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.22.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.22.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.23.attention.self.query.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.self.query.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.23.attention.self.key.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.self.key.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.attention.self.value.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.self.value.bias\n",
      "torch.Size([128, 128]): word_embedding_model.encoder.layer.23.attention.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.attention.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.23.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.23.output.bottleneck.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.output.bottleneck.dense.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.output.bottleneck.LayerNorm.bias\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.output.bottleneck.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.bottleneck.input.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.input.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.input.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.input.LayerNorm.weight\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.bottleneck.attention.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.attention.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.attention.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.bottleneck.attention.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.23.ffn.0.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.ffn.0.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.ffn.0.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.0.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.0.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.0.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.23.ffn.1.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.ffn.1.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.ffn.1.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.1.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.1.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.1.output.LayerNorm.weight\n",
      "torch.Size([512, 128]): word_embedding_model.encoder.layer.23.ffn.2.intermediate.dense.weight\n",
      "torch.Size([512]): word_embedding_model.encoder.layer.23.ffn.2.intermediate.dense.bias\n",
      "torch.Size([128, 512]): word_embedding_model.encoder.layer.23.ffn.2.output.dense.weight\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.2.output.dense.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.2.output.LayerNorm.bias\n",
      "torch.Size([128]): word_embedding_model.encoder.layer.23.ffn.2.output.LayerNorm.weight\n",
      "torch.Size([1, 512]): encoder.linear.weight\n",
      "torch.Size([1]): encoder.linear.bias\n",
      "parameters: 1113\n"
     ]
    }
   ],
   "source": [
    "paramCount=0\n",
    "for name, param in model.named_parameters():\n",
    "    paramCount+=1\n",
    "    print(f\"{param.size()}: {name}\")\n",
    "print(f\"parameters: {paramCount}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  childrens() : module(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========word_embedding_model===========\n",
      "MobileBertModel(\n",
      "  (embeddings): MobileBertEmbeddings(\n",
      "    (word_embeddings): Embedding(30522, 128, padding_idx=0)\n",
      "    (position_embeddings): Embedding(512, 512)\n",
      "    (token_type_embeddings): Embedding(2, 512)\n",
      "    (embedding_transformation): Linear(in_features=384, out_features=512, bias=True)\n",
      "    (LayerNorm): NoNorm()\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (encoder): MobileBertEncoder(\n",
      "    (layer): ModuleList(\n",
      "      (0): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (2): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (3): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (5): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (6): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (7): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (8): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (10): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (11): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (12): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (13): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (14): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (15): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (16): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (17): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (18): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (19): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (20): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (21): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (22): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (23): MobileBertLayer(\n",
      "        (attention): MobileBertAttention(\n",
      "          (self): MobileBertSelfAttention(\n",
      "            (query): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (key): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (value): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (output): MobileBertSelfOutput(\n",
      "            (dense): Linear(in_features=128, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (intermediate): MobileBertIntermediate(\n",
      "          (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (intermediate_act_fn): ReLU()\n",
      "        )\n",
      "        (output): MobileBertOutput(\n",
      "          (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (LayerNorm): NoNorm()\n",
      "          (bottleneck): OutputBottleneck(\n",
      "            (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (bottleneck): Bottleneck(\n",
      "          (input): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "          (attention): BottleneckLayer(\n",
      "            (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "            (LayerNorm): NoNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): ModuleList(\n",
      "          (0): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (1): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "          (2): FFNLayer(\n",
      "            (intermediate): MobileBertIntermediate(\n",
      "              (dense): Linear(in_features=128, out_features=512, bias=True)\n",
      "              (intermediate_act_fn): ReLU()\n",
      "            )\n",
      "            (output): FFNOutput(\n",
      "              (dense): Linear(in_features=512, out_features=128, bias=True)\n",
      "              (LayerNorm): NoNorm()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pooler): MobileBertPooler()\n",
      ")\n",
      "===========pooling_model===========\n",
      "Pooling()\n",
      "===========encoder===========\n",
      "SimpleLinearClassifier(\n",
      "  (linear): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n",
      "===========loss_func===========\n",
      "BCEWithLogitsLoss()\n",
      "===============================\n",
      "module: 4\n"
     ]
    }
   ],
   "source": [
    "moduleCount = 0\n",
    "for name,layer in model.named_children():\n",
    "    moduleCount += 1\n",
    "    print(f\"==========={name}===========\")\n",
    "    print(layer)\n",
    "print(\"===============================\")\n",
    "print(f\"module: {moduleCount}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ! pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='dataset.xlsx'\n",
    "df = pd.read_excel(file_path, engine='openpyxl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2448"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = df.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_list=[]\n",
    "for row in df_list:\n",
    "    value=row[0].replace('\\n',' ').replace('  ',' ').strip()\n",
    "    question_list.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2448"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original=[]\n",
    "for row in df_list:\n",
    "    value=row[0].replace('\\n',' ').replace('  ',' ').strip()\n",
    "    original.append([value,row[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('csv/text_origin1.csv','w',newline='',encoding='utf-8') as f:\n",
    "    write = csv.writer(f) \n",
    "    write.writerows(original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[]\n",
    "for question in question_list:\n",
    "    result.append(model.predict(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('csv/text_result1.csv','w',newline='',encoding='utf-8') as f:\n",
    "    write = csv.writer(f) \n",
    "    for q,r in zip(question_list,result):\n",
    "        write.writerow([q,r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('csv/text_result_origin1.csv','w',newline='',encoding='utf-8') as f:\n",
    "    write = csv.writer(f) \n",
    "    for q,r,o in zip(question_list,result,original):\n",
    "        write.writerow([q,r,o[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.13.0\n",
      "Torchvision Version:  0.14.0+cpu\n"
     ]
    }
   ],
   "source": [
    "# ! pip install torchvision\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Device:', torch.device('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text,val_text,train_result,val_result  = train_test_split(question_list, result, test_size=0.3, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1713, 1713)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_text),len(train_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformersum",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6ff3b088e1688949de694613e1e099f15dd014e3ce77655f10c191d22fb9d95c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
